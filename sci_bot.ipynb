{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <font color='red'> SciBot trained with Wiki, selected text, pdf folder, or SciBERT \n\n### <font color='blue'> * NLTK to process the text\n    \n### <font color='blue'> * Scikit-learn or TensorFlow to train model \n\n### <font color='blue'> * Chatbot framework (ChatterBot or Rasa) to revieve user input and generate responses.\n    \n####    References: \n####         https://www.kaggle.com/code/rajkumarl/wiki-ir-chatbot\n####         https://github.com/allenai/scibert\n####         https://arxiv.org/abs/1903.10676\n####         https://arxiv.org/pdf/1908.08835.pdf","metadata":{}},{"cell_type":"markdown","source":"# <font color='green'> 1. If training chatbot with SciBert \n    \nSciBERT is a pre-trained transformer-based model that can be fine-tuned for various natural language understanding tasks, such as text classification, named entity recognition and question answering. \n\nHere is an example of Python code that demonstrates how to fine-tune a SciBERT model for a text classification task using the Hugging Face's Transformers library:","metadata":{}},{"cell_type":"markdown","source":"\nimport transformers\n\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n### Load the SciBERT model and its tokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(\"scibert-scivocab-cased\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"scibert-scivocab-cased\")\n\n### Prepare the training data\n### load, preprocess the training data\n\n### Fine-tune the SciBERT model on the training data\n\n### Integrate the fine-tuned model into a chatbot framework such as ChatterBot or Rasa\n \n \n This code uses the Hugging Face's Transformers library to load the pre-trained SciBERT model and tokenizer, it could be used to fine-tune the model using the training data. After fine-tuning the model, it can be integrated into a chatbot framework like Chatterbot or Rasa to handle user input and generate responses.\n\nPlease note that this is just a skeleton code, you will need to add the preprocessing of the data, loading the training dataset, fine-tuning the model and integrating it into a chatbot framework. Also, make sure to adjust the model name to the one you want to use (scibert-scivocab-cased in this example) and the number of classes you have in your dataset.","metadata":{}},{"cell_type":"markdown","source":"# <font color='green'> 2. If use a selected folder of pdf files to train\n\nimport os\n\nimport PyPDF2\n\nfrom nltk.stem import WordNetLemmatizer\n\nfrom nltk.tokenize import word_tokenize\n\n### Define the folder path where the PDF files are located\nfolder_path = 'training_data_folder'\n\n### Initialize an empty string to store the text from all PDF files\nall_text = \"\"\n\n### Iterate over all PDF files in the folder\n    \nfor filename in os.listdir(folder_path):\n    if filename.endswith('.pdf'):\n        # Open the PDF file\n        with open(os.path.join(folder_path, filename), 'rb') as file:\n            pdf_reader = PyPDF2.PdfFileReader(file)\n\n            # Extract the text from the PDF file\n            for page_num in range(pdf_reader.numPages):\n                page = pdf_reader.getPage(page_num)\n                text = page.extractText()\n\n                # Add the text to the overall text from all PDF files\n                all_text += text\n\n### Preprocess the text data\nlemmatizer = WordNetLemmatizer()\nwords = word_tokenize(all_text)\nwords = [lemmatizer.lemmatize(word) for word in words]\n\n### Use the preprocessed text to train a chatbot model (using a library like NLTK or scikit-learn)\n# ... (model training code here)\n\n### Integrate the trained model into a chatbot framework (like ChatterBot or Rasa)\n### ... (chatbot framework code here)","metadata":{}},{"cell_type":"markdown","source":"# <font color='green'> 3. If using a selected text file to train Sci_Bot\n\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\n### Reading text file\nfile_path = 'chatbot_training_data.txt'\nwith open(file_path, 'r') as file:\n    text = file.read()\n\n### Preprocessing the text \nlemmatizer = WordNetLemmatizer()\nwords = word_tokenize(text)\nwords = [lemmatizer.lemmatize(word) for word in words]\n\n### Then use NLTK or scikit-learn to train a chatbot\n \n### Integrate the trained model into ChatterBot or Rasa","metadata":{}},{"cell_type":"markdown","source":"# <font color='green'> 4. If using Wikipedia API\n\n### Creating API object\nwiki = wikipediaapi.Wikipedia(language='en',\n                              extract_format=wikipediaapi.ExtractFormat.WIKI)\n                              \n                              \n### Extracting text                              \npage_title = 'Chatbot'\npage = wiki.page(page_title)\nif page.exists():\n    print(page.text)\nelse:\n    print(\"Page does not exist\")\n\n### Then use NLTK or scikit-learn to train a chatbot\n \n### Integrate the trained model into ChatterBot or Rasa\n ","metadata":{}},{"cell_type":"markdown","source":"# <font color='green'> 5. Scaping without using Wikipedia API\n","metadata":{}},{"cell_type":"code","source":"# To scrape Wikipedia\nfrom bs4 import BeautifulSoup\n# To access contents from URLs\nimport requests\n# to preprocess text\nimport nltk\n# to handle punctuations\nfrom string import punctuation\n# TF-IDF vectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# cosine similarity score\nfrom sklearn.metrics.pairwise import cosine_similarity \n# to do array operations\nimport numpy as np\n# to have sleep option\nfrom time import sleep \n\n#nltk.download('stopwords')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-23T17:15:49.161598Z","iopub.execute_input":"2023-01-23T17:15:49.162103Z","iopub.status.idle":"2023-01-23T17:15:51.141401Z","shell.execute_reply.started":"2023-01-23T17:15:49.161971Z","shell.execute_reply":"2023-01-23T17:15:51.140358Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color='green'> A multicomponent chatbot class \n### a. Initialization\n### b. Greetings from the Scibot\n### c. Chat function (called by user) that controls inputs, responses, data scraping, preprocessing, modeling.\n### d. Recieve_input from user \n### e. Respond from Sci_bot\n### f. Extracting information from Sources\n### g. Text Preprocessing","metadata":{}},{"cell_type":"code","source":"class ChatBot():\n    \n    # initialize bot\n    def __init__(self):\n        # flag whether to end chat\n        self.end_chat = False\n        # flag whether topic is found in wikipedia\n        self.got_topic = False\n        # flag whether to call respond()\n        # in some cases, response be made already\n        self.do_not_respond = True\n        \n        # wikipedia title\n        self.title = None\n        # wikipedia scraped para and description data\n        self.text_data = []\n        # data as sentences\n        self.sentences = []\n        # to keep track of paragraph indices\n        # corresponding to all sentences\n        self.para_indices = []\n        # currently retrieved sentence id\n        self.current_sent_idx = None\n        \n        # a punctuation dictionary\n        self.punctuation_dict = str.maketrans({p:None for p in punctuation})\n        # wordnet lemmatizer for preprocessing text\n        self.lemmatizer = nltk.stem.WordNetLemmatizer()\n        # collection of stopwords\n        self.stopwords = nltk.corpus.stopwords.words('english')\n        # initialize chatting\n        self.greeting()\n\n    # greeting method - to be called internally\n    # chatbot initializing chat on screen with greetings\n    def greeting(self):\n        print(\"Initializing ChatBot ...\")\n        # some time to get user ready\n        sleep(2)\n        # chat ending tags\n        print('Type \"bye\" or \"quit\" or \"exit\" to end chat')\n        sleep(2)\n        # chatbot descriptions\n        print('\\nEnter your topic of interest when prompted. \\\n        \\nChaBot will access Wikipedia, prepare itself to \\\n        \\nrespond to your queries on that topic. \\n')\n        sleep(3)\n        print('ChatBot will respond with short info. \\\n        \\nIf you input \"more\", it will give you detailed info \\\n        \\nYou can also jump to next query')\n        # give time to read what has been printed\n        sleep(3)\n        print('-'*50)\n        # Greet and introduce\n        greet = \"Hello, Great day! Please give me a topic of your interest. \"\n        print(\"ChatBot >>  \" + greet)\n        \n    # chat method - should be called by user\n    # chat method controls inputs, responses, data scraping, preprocessing, modeling.\n    # once an instance of ChatBot class is initialized, chat method should be called\n    # to do the entire chatting on one go!\n    def chat(self):\n        # continue chat\n        while not self.end_chat:\n            # receive input\n            self.receive_input()\n            # finish chat if opted by user\n            if self.end_chat:\n                print('ChatBot >>  See you soon! Bye!')\n                sleep(2)\n                print('\\nQuitting ChatBot ...')\n            # if data scraping successful\n            elif self.got_topic:\n                # in case not already responded\n                if not self.do_not_respond:\n                    self.respond()\n                # clear flag so that bot can respond next time\n                self.do_not_respond = False\n    \n    # receive_input method - to be called internally\n    # recieves input from user and makes preliminary decisions\n    def receive_input(self):\n        # receive input from user\n        text = input(\"User    >> \")\n        # end conversation if user wishes so\n        if text.lower().strip() in ['bye', 'quit', 'exit']:\n            # turn flag on \n            self.end_chat=True\n        # if user needs more information \n        elif text.lower().strip() == 'more':\n            # respond here itself\n            self.do_not_respond = True\n            # if at least one query has been received \n            if self.current_sent_idx != None:\n                response = self.text_data[self.para_indices[self.current_sent_idx]]\n            # prompt user to start querying\n            else:\n                response = \"Please input your query first!\"\n            print(\"ChatBot >>  \" + response)\n        # if topic is not chosen\n        elif not self.got_topic:\n            self.scrape_wiki(text)\n        else:\n            # add user input to sentences, so that we can vectorize in whole\n            self.sentences.append(text)\n                \n    # respond method - to be called internally\n    def respond(self):\n        # tf-idf-modeling\n        vectorizer = TfidfVectorizer(tokenizer=self.preprocess)\n        # fit data and obtain tf-idf vector\n        tfidf = vectorizer.fit_transform(self.sentences)\n        # calculate cosine similarity scores\n        scores = cosine_similarity(tfidf[-1],tfidf) \n        # identify the most closest sentence\n        self.current_sent_idx = scores.argsort()[0][-2]\n        # find the corresponding score value\n        scores = scores.flatten()\n        scores.sort()\n        value = scores[-2]\n        # if there is matching sentence\n        if value != 0:\n            print(\"ChatBot >>  \" + self.sentences[self.current_sent_idx]) \n        # if no sentence is matching the query\n        else:\n            print(\"ChatBot >>  I am not sure. Sorry!\" )\n        # remove the user query from sentences\n        del self.sentences[-1]\n        \n    # scrape_wiki method - to be called internally.\n    # called when user inputs topic of interest.\n    # employs requests to access Wikipedia via URL.\n    # employs BeautifulSoup to scrape paragraph tagged data\n    # and h1 tagged article heading.\n    # employs NLTK to tokenize data\n    def scrape_wiki(self,topic):\n        # process topic as required by Wikipedia URL system\n        topic = topic.lower().strip().capitalize().split(' ')\n        topic = '_'.join(topic)\n        try:\n            # creata an url\n            link = 'https://en.wikipedia.org/wiki/'+ topic\n            # access contents via url\n            data = requests.get(link).content\n            # parse data as soup object\n            soup = BeautifulSoup(data, 'html.parser')\n            # extract all paragraph data\n            # scrape strings with html tag 'p'\n            p_data = soup.findAll('p')\n            # scrape strings with html tag 'dd'\n            dd_data = soup.findAll('dd')\n            # scrape strings with html tag 'li'\n            #li_data = soup.findAll('li')\n            p_list = [p for p in p_data]\n            dd_list = [dd for dd in dd_data]\n            #li_list = [li for li in li_data]\n            # iterate over all data\n            for tag in p_list+dd_list: #+li_list:\n                # a bucket to collect processed data\n                a = []\n                # iterate over para, desc data and list items contents\n                for i in tag.contents:\n                    # exclude references, superscripts, formattings\n                    if i.name != 'sup' and i.string != None:\n                        stripped = ' '.join(i.string.strip().split())\n                        # collect data pieces\n                        a.append(stripped)\n                # with collected string pieces formulate a single string\n                # each string is a paragraph\n                self.text_data.append(' '.join(a))\n            \n            # obtain sentences from paragraphs\n            for i,para in enumerate(self.text_data):\n                sentences = nltk.sent_tokenize(para)\n                self.sentences.extend(sentences)\n                # for each sentence, its para index must be known\n                # it will be useful in case user prompts \"more\" info\n                index = [i]*len(sentences)\n                self.para_indices.extend(index)\n            \n            # extract h1 heading tag from soup object\n            self.title = soup.find('h1').string\n            # turn respective flag on\n            self.got_topic = True\n            # announce user that chatbot is ready now\n            print('ChatBot >>  Topic is \"Wikipedia: {}\". Let\\'s chat!'.format(self.title)) \n        # in case of unavailable topics\n        except Exception as e:\n            print('ChatBot >>  Error: {}. \\\n            Please input some other topic!'.format(e))\n        \n    # preprocess method - to be called internally by Tf-Idf vectorizer\n    # text preprocessing, stopword removal, lemmatization, word tokenization\n    def preprocess(self, text):\n        # remove punctuations\n        text = text.lower().strip().translate(self.punctuation_dict) \n        # tokenize into words\n        words = nltk.word_tokenize(text)\n        # remove stopwords\n        words = [w for w in words if w not in self.stopwords]\n        # lemmatize \n        return [self.lemmatizer.lemmatize(w) for w in words]\n","metadata":{"execution":{"iopub.status.busy":"2023-01-23T17:15:51.143836Z","iopub.execute_input":"2023-01-23T17:15:51.144152Z","iopub.status.idle":"2023-01-23T17:15:51.175225Z","shell.execute_reply.started":"2023-01-23T17:15:51.144110Z","shell.execute_reply":"2023-01-23T17:15:51.174003Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### <font color='green'> Start a Chat on Machine Learning Now ...","metadata":{}},{"cell_type":"code","source":"wiki = ChatBot()\n# call chat method\nwiki.chat()","metadata":{"execution":{"iopub.status.busy":"2023-01-23T17:15:51.176562Z","iopub.execute_input":"2023-01-23T17:15:51.176908Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Initializing ChatBot ...\nType \"bye\" or \"quit\" or \"exit\" to end chat\n\nEnter your topic of interest when prompted.         \nChaBot will access Wikipedia, prepare itself to         \nrespond to your queries on that topic. \n\nChatBot will respond with short info.         \nIf you input \"more\", it will give you detailed info         \nYou can also jump to next query\n--------------------------------------------------\nChatBot >>  Hello, Great day! Please give me a topic of your interest. \n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User    >>  machine learning\n"},{"name":"stdout","text":"ChatBot >>  Topic is \"Wikipedia: Machine learning\". Let's chat!\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User    >>  ok\n"},{"name":"stdout","text":"ChatBot >>  I am not sure. Sorry!\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User    >>  what is machine learning\n"},{"name":"stdout","text":"ChatBot >>  A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User    >>  how old is machine learning\n"},{"name":"stdout","text":"ChatBot >>  A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User    >>  branches of machine learning\n"},{"name":"stdout","text":"ChatBot >>  The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the Probably Approximately Correct Learning (PAC) model.\n","output_type":"stream"}]}]}